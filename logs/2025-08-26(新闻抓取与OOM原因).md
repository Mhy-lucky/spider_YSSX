
# 今日工作日志（2025-08-26）

**项目/任务**：人民网英语频道文章抓取与单语句保存与内存爆炸分析

---

## 1️⃣ 文章抓取

### 目标

1. **整理文章列表与 URL**

   * 收集并整理人民网所有目标频道的 URL，包括首页和各分类频道。
   * 确认 URL 中存在分页规则，如 `index.html` → `index2.html`、`index3.html` 等。

2. **搭建 Selenium 爬虫框架**

   * 使用 Selenium + ChromeDriver 自动打开网页、获取文章列表。
   * 支持断点续爬：爬取进度保存在 `progress.json`，可从上次中断页继续。
   * 每篇文章按标题生成文件夹，文章内容按句子保存。

3. **文章正文提取**

   * 针对文章页面 HTML 结构 `<div class="w860 d2txtCon cf">` 提取 `<p>` 段落内容。
   * 忽略图片 `<img>` 标签，避免抓取图片描述文本。

4. **单语句拆分逻辑优化**

   * 使用 `nltk.sent_tokenize` 处理文章文本，解决原正则切分存在的问题：

     * 日期缩写（如 `Aug. 8, 2025.`）不被错误拆分
     * 短句（如 `Strongly recommended.`）保留完整
   * 新增**句子长度过滤**：剔除小于 5 个单词的句子，提高语料质量。

5. **翻页逻辑修正**

   * 修正原有拼接分页 URL 的错误（如 `indexindex2.html`），改为：

     ```python
     if channel_url.endswith("index.html"):
         url = channel_url.replace("index.html", f"index{page_index}.html")
     else:
         url = channel_url.rstrip("/") + f"/index{page_index}.html"
     ```
   * 保证翻页链接正确，不产生重复或错误 URL。

6. **输出与存储**

   * 每篇文章按标题命名文件，每行保存一句话
   * 支持随时断点续爬，避免重复抓取
   * 所有操作均打印日志，便于追踪抓取进度与错误

---

### 问题与解决

| 问题                               | 解决方法                             |
| -------------------------------- | -------------------------------- |
| 日期缩写和短句被正则切分错误                   | 使用 `nltk.sent_tokenize` 替换正则拆分   |
| 短句（<5个词）影响语料质量                   | 新增过滤规则，保留 ≥5 个单词的句子              |
| 翻页 URL 拼接错误 (`indexindex2.html`) | 判断原 URL 是否为 `index.html`，按规则拼接分页 |
| 图片段落干扰正文                         | `<p>` 内含 `<img>` 自动跳过            |

---

### 后续改进方向

1. 自动检测每个频道的最大页数，避免手动翻页到空页
2. 优化爬虫速度，减少 `time.sleep()` 等待时间
3. 增加异常重试机制，保证网络波动时爬虫稳定
4. 可将结果导出为 JSON 或数据库，方便后续语料处理


## 2️⃣ 小说数据清洗内存爆炸分析

### 最直接的原因

1. **`seen_hashes` 集合无限增长**

   * 罪魁祸首代码： `seen_hashes.add(text_hash)`
   * 原脚本每行文本都会计算 hash 并存入 Python 的 `set` 中。
   * Python `set` 内部实现为哈希表，随着数据量增加，内存开销不仅是 hash 本身，还包括哈希表的额外空间。
   * 对于几十亿条数据，这会直接导致 **内存占用达到几十 GB**，最终 OOM。

2. **大文件逐行处理时临时对象累积**

   * `normalize_text`、正则匹配等操作会频繁产生新的字符串对象。
   * 在垃圾回收不及时的情况下，也会增加瞬时内存压力。

---

### 现有规避方式

1. **增量哈希持久化到磁盘 (`seen_hashes.txt`)**

   * 每次处理一行，先从磁盘文件读取已存在的 hash 集合（或仅追加新的 hash），避免一次性在内存中保存所有 hash。
   * 这样，即使处理数十亿行，也只需要维护 **最近批次的 hash 在内存中**，大幅降低内存占用。

2. **追加写入输出文件**

   * 清洗结果直接写入文件，不在内存中保存大量行。
   * 保持逐行处理，避免一次性加载大块数据。

---

### 后续可改进的地方

1. **使用更高效的磁盘存储 hash**

   * 例如 `sqlite`、`lmdb`、`LevelDB`，支持索引和快速查找，性能比纯文本文件更高。

2. **分块/分文件处理**

   * 将原始文件拆成若干小块，清洗完一个块释放内存，再处理下一个块。
   * 可结合多进程并行清洗，提高速度同时控制内存。

3. **压缩存储 hash**

   * 如果 hash 碰撞率可以接受，使用 Bloom Filter 或 MurmurHash 来替代完整 hash 集合。
   * 优点：内存占用可降到原来的 1/10 或更低，但存在极小概率误判。

4. **优化中间对象**

   * 避免重复生成字符串和正则对象，正则编译一次复用，`normalize` 函数返回尽量直接操作原对象。

---


---

📌 **今日结论**：
已完成人民网英语频道的单语抓取框架，支持按句子保存、过滤短句，并修复了翻页与句子切分问题。框架可稳定运行，支持断点续爬。同时针对大文件清洗去重逻辑分析了内存爆炸原因，提供了磁盘增量存储、分块处理及 Bloom Filter 等规避方案。

---

