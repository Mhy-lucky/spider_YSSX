# 工作日志（2025-09-01）

## 项目背景

目标是抓取 `http://en.people.cn/102775/` （习大大为中心）网站各主频道及其子频道的文章 URL，包括支持分页和多子频道的复杂结构，确保数据完整且可断点续爬。

---

## 1. 初始爬虫结构

* 使用 **Selenium + BeautifulSoup** 抓取文章链接。
* 初始思路：

  * 从"http://en.people.cn/102775/"找到所有主频道链接，保存在本地 `urls.txt`。
  * 从本地 `urls.txt` 读取主频道 URL。
  * 进入主频道页面提取文章 URL。
  * 遍历子频道，递归抓取分页文章。
  * 输出所有文章 URL 到 `all_articles.txt`。

**问题**：

* 子频道链接结构复杂，有些是图片列表、文章列表、外站链接，初版代码没能正确处理递增子频道。
* 爬到的 URL 有很多乱跳的情况，未严格按照主频道编号递增。

---

## 2. 调整子频道递增逻辑

* 改用**严格基于主频道编号递增**生成子频道：

  * 主频道 URL：`http://en.people.cn/102775/518922/index.html`
  * 子频道 URL：`http://en.people.cn/102775/518922/518923/index.html`、518924 等
* 通过正则提取主频道编号，按主频道编号 +1 递增生成子频道，避免跨主频道跳转。

**问题**：

* 子频道数量不确定，有些只有 1\~2 个，如果一直递增容易访问不存在的页面。
* 初版数字递增逻辑导致跨主频道抓取，结果混乱。

---

## 3. 边爬边写 + 翻页处理

* 修改 `crawl_subchannel`：

  * 进入子频道页面抓取文章 URL。
  * 支持翻页（点击 `Next`）。
  * **边爬边写**：每抓到一页就写入 `all_articles.txt`，避免内存占用过大。
* 解决了文章抓取量大时内存爆掉的问题。

---

## 4. 断点续爬实现

* 加入 `load_visited(output_file)`：

  * 读取 `all_articles.txt` 中已抓取 URL。
  * 已抓取 URL 加入 `visited_articles` 集合。
  * 后续抓取时跳过集合中已有 URL。
* 断点续爬效果：

  * 停止或重启爬虫，已抓取文章不会重复写入。
  * 保证长时间爬取任务安全可靠。

**注意**：

* 如果想从零开始重爬，需要先删除或重命名 `all_articles.txt`。

---

## 5. 完整抓取逻辑调整

* 获取子频道链接时：

  1. 先抓取主频道页面中真实存在的子频道链接。
  2. 如果子频道数量少于一定阈值（如 5），再尝试数字递增生成少量子频道。
* 保证：

  * 主频道处理完再处理下一个主频道。
  * 子频道严格基于当前主频道编号递增。
  * 支持图片形式文章子频道。
  * 避免跨站点或外链乱抓。

---

## 6. 测试和输出情况

* 测试 URL 示例：

  * 主频道：`http://en.people.cn/102775/518922/index.html`
  * 子频道递增：518923、518924 等
* 输出文件 `all_articles.txt`：

  * 边爬边写。
  * 已抓取文章会被加载，保证断点续爬。
* 遇到问题：

  * 初版爬虫输出文章数为 0（原因：网页结构复杂，原始提取规则未匹配）。
  * 子频道乱跳问题修正后，输出子频道严格按主频道编号递增。

---

## 7. 总结与下一步改进建议

**完成的改进**：

1. 边爬边写文章 URL，降低内存占用。
2. 支持断点续爬，从中断处继续抓取。
3. 子频道 URL 生成逻辑严格基于主频道编号递增。
4. 支持翻页抓取，兼容图片形式文章列表。

**待优化**：

* 自动判断子频道是否为图片列表或普通文章列表，适配不同页面结构。
* 针对外站、非标准域名链接进行过滤。
* 增加日志记录，抓取异常页面或网络错误时可回溯。
* 可并行爬取多个主频道以加快抓取速度（需注意 Selenium 会话管理）。

---

## 8.最重要的发现：
* 基于网站内主频道数量多，且各主频道内页面结构多样复杂，无法通过找规律写代码来循环获取所有主频道的全部页面，
偶然发现可以通过严格基于主频道url编号递增生成子频道url，就能把主频道和子频道内所有文章都抓到。
* 人民网前面的所有频道都已经处理完了，是通过找页面结构规律循环处理的，习大大频道，没法找规律，只能通过递增编号的方式来处理。但其实前面的频道也可以采用编号递增来构建URL，不用去找页面的复杂规律。
* 静态页面其实可以采用scrapy的方式来爬取，selenium的方式更适合动态页面。（基于时间有限，未改用scrapy）