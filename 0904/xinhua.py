import asyncio
import json
import os
from playwright.async_api import async_playwright

CHANNEL_FILE = "/Users/admin/Desktop/coding/0904/channels.json"  # é¢‘é“ JSONï¼Œæ¯è¡Œä¸€ä¸ª URL æˆ– ["channels"]
ARTICLES_FILE = "articles.txt"
FAILED_FILE = "failed.txt"
PROGRESS_FILE = "progress.json"  # è®°å½•å·²çˆ¬å–é¢‘é“

async def crawl_channel(page, url):
    await page.goto(url, timeout=60000)
    article_urls = set()

    while True:
        # è·å–æ–‡ç« åˆ—è¡¨é“¾æ¥
        links = await page.locator("#list div.tit a").all()
        for l in links:
            try:
                href = await l.get_attribute("href")
                if href:
                    if href.startswith(".."):
                        href = "https://english.news.cn" + href.replace("..", "")
                    elif href.startswith("/"):
                        href = "https://english.news.cn" + href
                    article_urls.add(href)
            except:
                pass

        # ç‚¹å‡» More
        more_btn = page.locator("#more.list-more")
        if await more_btn.count() == 0 or not await more_btn.is_visible():
            break
        try:
            await more_btn.click()
            await page.wait_for_timeout(2000)
        except:
            break

    return list(article_urls)

async def main():
    # è¯»å–è¿›åº¦
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            progress = json.load(f)
    else:
        progress = {"done": [], "failed": []}

    # è¯»å–é¢‘é“åˆ—è¡¨
    with open(CHANNEL_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
        # å…¼å®¹ JSON æ˜¯ list æˆ– {"channels": [...]}
        channels = data.get("channels") if isinstance(data, dict) else data

    articles_out = open(ARTICLES_FILE, "a", encoding="utf-8")
    failed_out = open(FAILED_FILE, "a", encoding="utf-8")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        for url in channels:
            if url in progress["done"] or url in progress["failed"]:
                print(f"â­ å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ {url}")
                continue

            print(f"ğŸ“‘ Crawling {url}")
            try:
                articles = await crawl_channel(page, url)
                for a in articles:
                    articles_out.write(a + "\n")
                articles_out.flush()

                progress["done"].append(url)
                print(f"âœ… æˆåŠŸæŠ“åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            except Exception as e:
                print(f"âŒ å¤±è´¥: {url} ({e})")
                failed_out.write(url + "\n")
                failed_out.flush()
                progress["failed"].append(url)

            # ä¿å­˜è¿›åº¦
            with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)

        await browser.close()

    articles_out.close()
    failed_out.close()
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œæ–‡ç« å­˜åˆ° {ARTICLES_FILE}ï¼Œå¤±è´¥é¢‘é“å­˜åˆ° {FAILED_FILE}")

if __name__ == "__main__":
    asyncio.run(main())
