import asyncio
import json
import os
from playwright.async_api import async_playwright

CHANNEL_FILE = "/Users/admin/Desktop/coding/0904/failed.txt"
ARTICLES_FILE = "/Users/admin/Desktop/coding/0904/articles.txt"
FAILED_FILE = "/Users/admin/Desktop/coding/0904/failed_1.txt"
PROGRESS_FILE = "/Users/admin/Desktop/coding/0904/progress.txt"
MORE_RETRY = 3  # ç‚¹å‡» More å¤±è´¥æ—¶é‡è¯•æ¬¡æ•°
WAIT_AFTER_CLICK = 2000  # ç‚¹å‡» More åç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰


async def crawl_channel(page, url, articles_out):
    await page.goto(url, timeout=60000)
    article_urls = set()

    while True:
        # è·å–æ–‡ç« åˆ—è¡¨é“¾æ¥
        links = await page.locator("#list div.tit a").all()
        new_links = 0
        for l in links:
            try:
                href = await l.get_attribute("href")
                if href:
                    if href.startswith(".."):
                        href = "https://english.news.cn" + href.replace("..", "")
                    elif href.startswith("/"):
                        href = "https://english.news.cn" + href
                    if href not in article_urls:
                        article_urls.add(href)
                        new_links += 1
            except:
                pass

        # åªå†™å…¥æ–°å¢æ–‡ç« 
        if new_links > 0:
            for link in list(article_urls)[-new_links:]:
                articles_out.write(link + "\n")
            articles_out.flush()

        print(f"ğŸ“ {new_links} new articles found on this page of {url}")

        # ç‚¹å‡» More å¹¶é‡è¯•
        more_btn = page.locator("#more.list-more")
        if await more_btn.count() == 0 or not await more_btn.is_visible():
            break

        success = False
        for attempt in range(MORE_RETRY):
            try:
                await more_btn.click()
                await page.wait_for_timeout(WAIT_AFTER_CLICK)
                success = True
                break
            except Exception as e:
                print(f"âš ï¸ ç‚¹å‡» More å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{MORE_RETRY}: {e}")
                await page.wait_for_timeout(1000)

        # ç‚¹å‡» More åå¦‚æœæ²¡æœ‰æ–°æ–‡ç« ï¼Œåˆ™è·³åˆ°ä¸‹ä¸€ä¸ªé¢‘é“
        if not success or new_links == 0:
            print(f"ğŸ›‘ ç‚¹å‡» More åæ²¡æœ‰æ–°æ–‡ç« ï¼Œåœæ­¢ {url}")
            break

    return list(article_urls)


async def main():
    # è¯»å–è¿›åº¦
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            progress = json.load(f)
    else:
        progress = {"done": [], "failed": []}

        # è¯»å–é¢‘é“åˆ—è¡¨
    with open(CHANNEL_FILE, "r", encoding="utf-8") as f:
        channels = [line.strip() for line in f if line.strip()]

    articles_out = open(ARTICLES_FILE, "a", encoding="utf-8")
    failed_out = open(FAILED_FILE, "a", encoding="utf-8")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        for url in channels:
            if url in progress["done"] or url in progress["failed"]:
                print(f"â­ å·²å¤„ç†è¿‡ï¼Œè·³è¿‡ {url}")
                continue

            print(f"ğŸ“‘ Crawling {url}")
            try:
                await crawl_channel(page, url, articles_out)
                progress["done"].append(url)
                print(f"âœ… å®Œæˆé¢‘é“ {url}")
            except Exception as e:
                print(f"âŒ å¤±è´¥: {url} ({e})")
                failed_out.write(url + "\n")
                failed_out.flush()
                progress["failed"].append(url)

            # ä¿å­˜è¿›åº¦
            with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)

        await browser.close()

    articles_out.close()
    failed_out.close()
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œæ–‡ç« å­˜åˆ° {ARTICLES_FILE}ï¼Œå¤±è´¥é¢‘é“å­˜åˆ° {FAILED_FILE}")


if __name__ == "__main__":
    asyncio.run(main())
